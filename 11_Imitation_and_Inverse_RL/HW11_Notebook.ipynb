{
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "id": "cfbbc405",
   "cell_type": "markdown",
   "source": [
    "# HW11: Imitation Learning\n",
    "\n",
    "> - Full Name: **Danial Parnian**\n",
    "> - Student ID: **401110307**\n",
    "<!--\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DeepRLCourse/Homework-5-Questions/blob/main/RL_HW11_Dyna.ipynb)\n",
    "[![Open In kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/DeepRLCourse/Homework-11-Questions/main/RL_HW5_Dyna.ipynb) -->\n",
    "\n",
    "## Overview\n",
    "This assignment investigates the strengths and limitations of both reinforcement learning (PPO, A2C) and imitation learning (DAgger, GAIL) by applying them to control tasks. Students train and evaluate agents using expert demonstrations and environment feedback, gaining hands-on experience in designing and comparing learning algorithms across paradigms.\n",
    "\n",
    "\n",
    "### **Exercise 1: Training PPO, A2C, and DAgger on CartPole-v1** (60 Points)\n",
    "\n",
    "This exercise focuses on comparing reinforcement learning (PPO, A2C) and imitation learning (DAgger) techniques on the CartPole-v1 environment. An expert policy is first trained using PPO, then used to guide a DAgger agent. Separately, an A2C agent is trained directly via interaction with the environment. Performance metrics are collected to assess how each approach learns and generalizes.\n",
    "\n",
    "---\n",
    "\n",
    "### **Exercise 2: Training a GAIL Model Using an Expert Policy** (100 Points)\n",
    "\n",
    "In this task, a GAIL (Generative Adversarial Imitation Learning) agent is trained to imitate a pre-trained expert policy in several environments (e.g., CartPole-v1, Pendulum-v0, BipedalWalker-v3). The training process involves configuring the environment, initializing models, and iteratively training the agent to match the expert’s behavior using adversarial learning.\n"
   ],
   "metadata": {
    "id": "cfbbc405"
   }
  },
  {
   "id": "811e8eac-bd35-4e4c-a337-c49dcf1867ce",
   "cell_type": "code",
   "source": [
    "!pip install stable_baselines3"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-08-04T05:14:59.732865Z",
     "iopub.execute_input": "2025-08-04T05:14:59.733136Z",
     "iopub.status.idle": "2025-08-04T05:16:14.387199Z",
     "shell.execute_reply.started": "2025-08-04T05:14:59.733115Z",
     "shell.execute_reply": "2025-08-04T05:16:14.386226Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "id": "811e8eac-bd35-4e4c-a337-c49dcf1867ce",
    "outputId": "b8824f07-850c-4b37-be32-a201d23ca5a6"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "2fdcb13a-9175-4a5e-b77d-63ab5b6e84d9",
   "cell_type": "code",
   "source": [
    "!pip install imitation"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-08-04T05:17:15.608946Z",
     "iopub.execute_input": "2025-08-04T05:17:15.609548Z",
     "iopub.status.idle": "2025-08-04T05:17:21.160526Z",
     "shell.execute_reply.started": "2025-08-04T05:17:15.609522Z",
     "shell.execute_reply": "2025-08-04T05:17:21.159564Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "id": "2fdcb13a-9175-4a5e-b77d-63ab5b6e84d9",
    "outputId": "fc36976b-ee24-4f26-a9ca-701b513a2e86"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "1f404622",
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO, DQN\n",
    "from imitation.util.util import make_vec_env\n",
    "from imitation.algorithms.dagger import SimpleDAggerTrainer\n",
    "from imitation.algorithms import bc\n",
    "from imitation.policies.serialize import load_policy\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import argparse\n",
    "import torch\n",
    "import tempfile\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO,DQN,A2C\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-08-04T05:17:26.051928Z",
     "iopub.execute_input": "2025-08-04T05:17:26.052219Z",
     "iopub.status.idle": "2025-08-04T05:17:27.157199Z",
     "shell.execute_reply.started": "2025-08-04T05:17:26.052194Z",
     "shell.execute_reply": "2025-08-04T05:17:27.156669Z"
    },
    "id": "1f404622"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "80429498",
   "cell_type": "markdown",
   "source": [
    "# Training PPO, A2C, and DAgger on the CartPole-v1 Environment\n",
    "\n",
    "In this section, we explore and compare multiple reinforcement learning and imitation learning approaches within the classic **CartPole-v1** environment. The algorithms used include:\n",
    "\n",
    "- **Proximal Policy Optimization (PPO)** – a stable, on-policy RL algorithm.\n",
    "- **Advantage Actor-Critic (A2C)** – another on-policy RL algorithm known for simplicity and efficiency.\n",
    "- **DAgger (Dataset Aggregation)** – an imitation learning algorithm that iteratively refines a policy using expert guidance.\n",
    "\n",
    "---\n",
    "\n",
    "## Main Objectives\n",
    "\n",
    "1. **Train an expert policy using PPO**.\n",
    "2. **Use the PPO expert to train a new agent via DAgger**.\n",
    "3. **Train a separate agent using A2C directly**.\n",
    "4. **Compare performance between PPO, DAgger, and A2C**.\n",
    "\n",
    "This setup allows us to analyze the strengths of:\n",
    "- **Reinforcement learning** (PPO and A2C) that learns from environment feedback.\n",
    "- **Imitation learning** (DAgger) that learns from an expert’s demonstrations and corrections.\n",
    "\n",
    "---"
   ],
   "metadata": {
    "id": "80429498"
   }
  },
  {
   "id": "e8482ab7",
   "cell_type": "markdown",
   "source": [
    "### 1. PPO Expert Training (15 Points)\n",
    "- A PPO agent is trained in the CartPole-v1 environment using `stable-baselines3`.\n",
    "- The model is saved and later reloaded as an expert policy for imitation learning.\n",
    "- Training is done in intervals and evaluated periodically to track average rewards."
   ],
   "metadata": {
    "id": "e8482ab7"
   }
  },
  {
   "id": "05fd824f",
   "cell_type": "code",
   "source": [
    "# Create the CartPole-v1 environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "regular_rewards = []\n",
    "\n",
    "# Training loop\n",
    "total_timesteps = 10000\n",
    "eval_freq = 1000\n",
    "\n",
    "for timestep in range(0, total_timesteps, eval_freq):\n",
    "    model.learn(total_timesteps=eval_freq)\n",
    "\n",
    "    # Evaluate the model and record the reward\n",
    "    reward, _ = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "    regular_rewards.append(reward)\n",
    "\n",
    "    print(f\"Regular Training - Step {timestep + eval_freq}: Mean Reward = {reward}\")\n",
    "\n",
    "model.save(\"CartPole_v1_PPO_model.zip\")\n",
    "env.close()"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-08-04T05:17:35.004050Z",
     "iopub.execute_input": "2025-08-04T05:17:35.004620Z",
     "iopub.status.idle": "2025-08-04T05:18:44.588964Z",
     "shell.execute_reply.started": "2025-08-04T05:17:35.004593Z",
     "shell.execute_reply": "2025-08-04T05:18:44.588114Z"
    },
    "id": "05fd824f",
    "outputId": "e5b8b4a3-155a-436e-8dbf-dd0416825d8f"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "bb516c6b",
   "cell_type": "markdown",
   "source": [
    "### 2. DAgger Training with the PPO Expert (25 Points)\n",
    "- A `BehaviorCloning` trainer is used to initialize the DAgger process.\n",
    "- A `SimpleDAggerTrainer` collects data from both the expert and the learner during training.\n",
    "- The model is updated incrementally and evaluated periodically.\n",
    "- Performance rewards are stored to track learning progress."
   ],
   "metadata": {
    "id": "bb516c6b"
   }
  },
  {
   "id": "6815b147",
   "cell_type": "code",
   "source": [
    "# Load the PPO model manually using Stable-Baselines3's load method\n",
    "expert = PPO.load(\"CartPole_v1_PPO_model.zip\")\n",
    "\n",
    "# Create vectorized environment for imitation learning\n",
    "rng = np.random.default_rng(0)\n",
    "venv = make_vec_env(\"CartPole-v1\", n_envs=1, rng=rng)\n",
    "\n",
    "# Initialize the Behavior Cloning (BC) trainer\n",
    "bc_trainer = bc.BC(\n",
    "    observation_space=venv.observation_space,\n",
    "    action_space=venv.action_space,\n",
    "    rng=rng,\n",
    "    policy=expert.policy\n",
    ")\n",
    "\n",
    "# Record rewards during DAgger training\n",
    "dagger_rewards = []\n",
    "\n",
    "# Train using DAgger\n",
    "with tempfile.TemporaryDirectory(prefix=\"dagger_example_\") as tmpdir:\n",
    "    dagger_trainer = SimpleDAggerTrainer(\n",
    "        venv=venv,\n",
    "        scratch_dir=tmpdir,\n",
    "        expert_policy=expert,\n",
    "        bc_trainer=bc_trainer,\n",
    "        rng=rng\n",
    "    )\n",
    "\n",
    "    # Training loop for DAgger\n",
    "    total_dagger_timesteps = 8000\n",
    "    eval_freq = 1000\n",
    "    for timestep in range(0, total_dagger_timesteps, eval_freq):\n",
    "        dagger_trainer.train(eval_freq)\n",
    "\n",
    "        # Evaluate the model and record the reward\n",
    "        reward, _ = evaluate_policy(dagger_trainer.policy, venv, n_eval_episodes=10)\n",
    "        dagger_rewards.append(reward)\n",
    "        print(f\"DAgger Training - Step {timestep + eval_freq}: Mean Reward = {reward}\")\n",
    "\n",
    "# Save the DAgger policy\n",
    "dagger_trainer.policy.save(\"CartPole_v1_DAgger_model.zip\")\n",
    "venv.close()"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-08-04T05:25:22.709800Z",
     "iopub.execute_input": "2025-08-04T05:25:22.710298Z",
     "iopub.status.idle": "2025-08-04T05:27:16.082607Z",
     "shell.execute_reply.started": "2025-08-04T05:25:22.710264Z",
     "shell.execute_reply": "2025-08-04T05:27:16.081897Z"
    },
    "colab": {
     "referenced_widgets": [
      "5aec1c02be684909bb27dadcd9e12329",
      "64316ef59a6b4f70a420be6dc5451aea",
      "ddcb9c4b72ff4f08a24e439d0cbc0391",
      "c8257b04617b4676bd5d96dd31e5baf7",
      "51cb671fdc254275b40171bfe84b81b0",
      "9f94df419a75479cbb7850b5eb7454e9",
      "6ee82c4f2654483b8d312189ba0bf890",
      "626fe38885dc41c3b37efcc896f91ee6",
      "5987ca5b239f481abb52e7fdeae0195e",
      "cd1123936c7d4f9d9a69e4393fbbada0",
      "41066238e1f24930a0d4e981c45e9693",
      "db26df9ce97040e487dd61c6a1752952",
      "d7b5f200ede747feb5275c26dc3cb110",
      "f8c2297fb3084e578045b9d7b96a960b",
      "3e5ea4bd45eb42f4b141d2ffc0679975",
      "eb3f556527e84263a7f819293b8724d2",
      "41a4c1cf09084de4ad752ca3e99ffc00",
      "08573c12ba8d42d08d196dc2082d7e34",
      "ad5dbb18ca3b4010a83f07fa859cecdc",
      "fa1f4cd85e1d4e8783b1ea4bdf2e3b93",
      "a9965784606140deb1014605ab245954",
      "517de5d28dfe4e7fa309c8d30a82ca2e",
      "aeeb188d30c347948c7ffb4f495569db",
      "7079840e763041c7a6d7db38f087d02e"
     ]
    },
    "id": "6815b147",
    "outputId": "0d042936-d111-4243-ead5-704fa89205ef"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "ca8db856",
   "cell_type": "code",
   "source": [
    "# Plot the rewards for Regular PPO Training and DAgger Training\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(0, 10000, 1000), regular_rewards, label=\"PPO Regular Training\", marker=\"o\")\n",
    "plt.plot(range(0, 8000, 1000), dagger_rewards, label=\"DAgger Training\", marker=\"o\")\n",
    "plt.xlabel(\"Timesteps\")\n",
    "plt.ylabel(\"Mean Reward\")\n",
    "plt.title(\"Comparison of PPO and DAgger Training Rewards\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-08-04T05:28:08.366762Z",
     "iopub.execute_input": "2025-08-04T05:28:08.367550Z",
     "iopub.status.idle": "2025-08-04T05:28:08.712088Z",
     "shell.execute_reply.started": "2025-08-04T05:28:08.367523Z",
     "shell.execute_reply": "2025-08-04T05:28:08.711059Z"
    },
    "id": "ca8db856",
    "outputId": "853f5cc8-196d-48a2-e989-005d275a55a3"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "46f18622",
   "cell_type": "markdown",
   "source": [
    "\n",
    "### 3. A2C Agent Training (20 Points)\n",
    "After DAgger training, a third agent is trained using the **Advantage Actor-Critic (A2C)** algorithm."
   ],
   "metadata": {
    "id": "46f18622"
   }
  },
  {
   "id": "c822cdb9",
   "cell_type": "code",
   "source": [
    "# Create the CartPole-v1 environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# Initialize the A2C model\n",
    "model = A2C(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# Record rewards during training\n",
    "regular_rewards = []\n",
    "\n",
    "# Training loop\n",
    "total_timesteps = 10000\n",
    "eval_freq = 1000\n",
    "for timestep in range(0, total_timesteps, eval_freq):\n",
    "    model.learn(total_timesteps=eval_freq)\n",
    "\n",
    "    # Evaluate the model and record the reward\n",
    "    reward, _ = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "    regular_rewards.append(reward)\n",
    "    print(f\"Regular Training - Step {timestep + eval_freq}: Mean Reward = {reward}\")\n",
    "\n",
    "# Save the model after training\n",
    "model.save(\"CartPole_v1_A2C_model.zip\")\n",
    "env.close()"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-08-04T05:31:15.004110Z",
     "iopub.execute_input": "2025-08-04T05:31:15.004419Z",
     "iopub.status.idle": "2025-08-04T05:31:50.799253Z",
     "shell.execute_reply.started": "2025-08-04T05:31:15.004391Z",
     "shell.execute_reply": "2025-08-04T05:31:50.798519Z"
    },
    "id": "c822cdb9",
    "outputId": "d9c6cfac-d199-466f-c97e-b8ab83a800df"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "59de5485",
   "cell_type": "code",
   "source": [
    "# Load the A2C model manually using Stable-Baselines3's load method\n",
    "expert = A2C.load(\"CartPole_v1_A2C_model.zip\")\n",
    "\n",
    "# Create vectorized environment for imitation learning\n",
    "rng = np.random.default_rng(0)\n",
    "venv = make_vec_env(\"CartPole-v1\", n_envs=1, rng=rng)\n",
    "\n",
    "# Initialize the Behavior Cloning (BC) trainer\n",
    "bc_trainer = bc.BC(\n",
    "    observation_space=venv.observation_space,\n",
    "    action_space=venv.action_space,\n",
    "    rng=rng,\n",
    "    policy=expert.policy\n",
    ")\n",
    "\n",
    "# Record rewards during DAgger training\n",
    "dagger_rewards = []\n",
    "\n",
    "# Train using DAgger\n",
    "with tempfile.TemporaryDirectory(prefix=\"dagger_example_\") as tmpdir:\n",
    "    dagger_trainer = SimpleDAggerTrainer(\n",
    "        venv=venv,\n",
    "        scratch_dir=tmpdir,\n",
    "        expert_policy=expert,\n",
    "        bc_trainer=bc_trainer,\n",
    "        rng=rng\n",
    "    )\n",
    "\n",
    "    # Training loop for DAgger\n",
    "    total_dagger_timesteps = 8000\n",
    "    eval_freq = 1000\n",
    "    for timestep in range(0, total_dagger_timesteps, eval_freq):\n",
    "        dagger_trainer.train(eval_freq)  # Train the agent for `eval_freq` timesteps\n",
    "\n",
    "        # Evaluate the model and record the reward\n",
    "        reward, _ = evaluate_policy(dagger_trainer.policy, venv, n_eval_episodes=10)\n",
    "        dagger_rewards.append(reward)\n",
    "        print(f\"DAgger Training - Step {timestep + eval_freq}: Mean Reward = {reward}\")\n",
    "\n",
    "# Close the environment after DAgger training\n",
    "venv.close()"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-08-04T05:31:50.800464Z",
     "iopub.execute_input": "2025-08-04T05:31:50.800716Z",
     "iopub.status.idle": "2025-08-04T05:33:16.970204Z",
     "shell.execute_reply.started": "2025-08-04T05:31:50.800698Z",
     "shell.execute_reply": "2025-08-04T05:33:16.969631Z"
    },
    "colab": {
     "referenced_widgets": [
      "2f00d64fc2e9462082ac25d4e26d9e44",
      "7d31a30152684c7896dadede886faf6f",
      "4eadf27fbea441dea98edd6b596188c6",
      "6979312177f74f568b3cadc6d59e1701",
      "45d180d650aa4ebb83fa84a4992cdac1",
      "103992d3325b4d228c34b25898382ec0",
      "b2087683feb24bfd870795cd4b835347",
      "54ba7e0d6e244cb3bba9862db51b881d",
      "5576a5ba1ea045799d520c93ceba98c4",
      "6007b308d188441e8219f721b5e912b1",
      "b4ea65102fac446d9fd57933aad47c6b",
      "bf972e136d8443b8b7883273bd31aa59",
      "d278dd07240e4b449f6494f3773962d9",
      "61ad4edbf7f74026b1ea71227265de79",
      "b8f8b06f942d4e47a4d50b8239db0b06",
      "f125a9730a254d53b0127fb05bedd3fe",
      "8662300f20f54968a46c1655e298dca5",
      "36b410133c804359b2da5b85e0d533cb",
      "5a9c9233776c453a9cbd5609d69e10d6",
      "345cb08fe4ae446d83cda04949fa78bc",
      "6bc036aadaf14abaabe216f3324fc85b",
      "9dbcca7e519d4b6186ff0cbd3beaf163",
      "bfeac30094a9431bb5c8487a642f4e06",
      "de3711ae23404cd38ad67883b7a4cdb5",
      "1481690d87044f5aa19062b923666735",
      "6b45900071044005aea0c4216530a517",
      "bd3950701f4a468cb3c7362e82cfae63",
      "abcc6b65fc354f2ba40a9c793a0fb268",
      "5b8deb21b1d7487caf5fa78d7caac8ba",
      "f748c4814c9c4385a1f14ab7bcf401e9",
      "42f818ff01b8460eb64250def526dd87",
      "205b1086823246f79a51286c1fadf596",
      "72fdf6fa4d7b48aa8032350873f19448",
      "fd356f484929456e90f454deaad06734",
      "5e96595f926d4a488d67a2c2ab281a34",
      "bafa92d9a62b41eba920d7de0257660a",
      "54aa121693224a22b803435eadbf91dd",
      "6afe07e137c74c82a83b96f25b35059c",
      "28f510e7ddb34cb788c57d36bbf5bea8",
      "a67e99850eb6405cb09cd7c9c97810af",
      "3165048f47304ba09ecc1c15221fd59c",
      "46481e5c32be467f857324eb83315a0f",
      "48096bb1604d4c1a96ee740f8f8cc8d0",
      "41e5f6d92c1743e89b55597ce8a59b75",
      "45f816bedce142de81dbb90ace2b727e",
      "74d3bd26e8424c808bf5afecee234dbe",
      "b1f117207023449cbd3800b50d123d96",
      "9f980acddc20416abbe3dd4f8c52297b",
      "67641b7463be4838b5d296658029196c",
      "9d29a5b1f4d94ba7adffa15b88183c24",
      "e884fc67cf8942d08daf54f753a1b127",
      "e03d8e219a29453586a9560ef70b3f62",
      "9db197810b4e49e0af01bc5d9f7fba11",
      "7719f847e9714357adf1ac7273bd4510",
      "13b9ef03dcbe4ab6bc5e8cf4e2dcb589",
      "cd16820e31814bde8749269fef308a0d",
      "e752778571da495e9309987c8a21bf63",
      "1671ccb55d12454c9262d1d3307b1a55",
      "911ae1f0adfe4fb1a85cec1741574e02",
      "e86fc264ecec4e768fb46795c01ec465",
      "be658467abba4886b13ad183616a17b8",
      "8909309c56eb4e6ba978d6478abf3eb4",
      "e35c00f99e7a489dbcfd9d95bb927c5b"
     ]
    },
    "id": "59de5485",
    "outputId": "30b399ff-f417-4c2b-80b5-8b4198d2279f"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "b3e844a7",
   "cell_type": "code",
   "source": [
    "# Plot the rewards for Regular A2C Training and DAgger Training\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(0, 10000, 1000), regular_rewards, label=\"A2C Regular Training\", marker=\"o\")\n",
    "plt.plot(range(0, 8000, 1000), dagger_rewards, label=\"DAgger Training\", marker=\"o\")\n",
    "\n",
    "plt.xlabel(\"Timesteps\")\n",
    "plt.ylabel(\"Mean Reward\")\n",
    "plt.title(\"Comparison of A2C and DAgger Training Rewards\")\n",
    "plt.legend()\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-08-04T05:33:16.970902Z",
     "iopub.execute_input": "2025-08-04T05:33:16.971142Z",
     "iopub.status.idle": "2025-08-04T05:33:17.144011Z",
     "shell.execute_reply.started": "2025-08-04T05:33:16.971123Z",
     "shell.execute_reply": "2025-08-04T05:33:17.143330Z"
    },
    "id": "b3e844a7",
    "outputId": "507a1abf-9d86-492c-a4d6-66e11e3e40d1"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "efd02210",
   "cell_type": "markdown",
   "source": [
    "# Training a GAIL Model Using an Expert Policy\n",
    "\n",
    "In this section, we walk through the process of setting up and training a **Generative Adversarial Imitation Learning (GAIL)** model using a pre-trained expert in environments such as `CartPole-v1`, `Pendulum-v0`, and `BipedalWalker-v3`.\n",
    "\n",
    "The full pipeline includes:\n",
    "- Setting up directories and configurations,\n",
    "- Loading and validating the environment,\n",
    "- Initializing expert and GAIL models,\n",
    "- Training GAIL using the expert’s behavior,\n",
    "- Saving the final results and model weights.\n",
    "\n",
    "## Summary\n",
    "\n",
    "This pipeline implements a complete imitation learning workflow using GAIL. The process involves:\n",
    "- Setting up a Gym-compatible environment,\n",
    "- Loading a pre-trained expert model,\n",
    "- Training a GAIL agent to mimic the expert,\n",
    "- Storing the training results and model artifacts.\n",
    "\n",
    "By following this structure, we can compare GAIL's performance to reinforcement learning approaches like PPO or A2C, and evaluate the efficiency of learning from demonstrations.\n",
    "\n",
    "---\n"
   ],
   "metadata": {
    "id": "efd02210"
   }
  },
  {
   "id": "4cf3e0d3",
   "cell_type": "markdown",
   "source": [
    "### **1. `setup_directories(env_name)`** (5 Points)\n",
    "\n",
    "```python\n",
    "This function handles directory setup and expert configuration:\n",
    "- Creates a base checkpoint directory (`ckpts/`).\n",
    "- Validates if the provided environment name is supported.\n",
    "- Loads the expert model configuration from a JSON file.\n",
    "- Creates a subdirectory for storing environment-specific checkpoints.\n",
    "\n",
    "**Purpose:** Prepare the file system and load settings for the expert model."
   ],
   "metadata": {
    "id": "4cf3e0d3"
   }
  },
  {
   "id": "6bf4252e-20fd-444d-a5fa-13e51d16d66b",
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import argparse\n",
    "import torch\n",
    "import gym\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "\n",
    "# First, unzip the utils folder\n",
    "with zipfile.ZipFile('utils.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('.')\n",
    "\n",
    "# Add utils directory to Python path and import necessary modules\n",
    "sys.path.append('./utils')\n",
    "\n",
    "from nets import *\n",
    "from gail import *\n",
    "from funcs import *"
   ],
   "metadata": {
    "trusted": true,
    "id": "6bf4252e-20fd-444d-a5fa-13e51d16d66b"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "6a08ebd1",
   "cell_type": "code",
   "source": [
    "def setup_directories(env_name):\n",
    "    # Create checkpoint directory\n",
    "    ckpt_path = \"ckpts\"\n",
    "    if not os.path.isdir(ckpt_path):\n",
    "        os.mkdir(ckpt_path)\n",
    "\n",
    "    # Validate environment name\n",
    "    if env_name not in [\"CartPole-v1\", \"Pendulum-v0\", \"BipedalWalker-v3\"]:\n",
    "        print(\"The environment name is wrong!\")\n",
    "        return None\n",
    "\n",
    "    # Set expert checkpoint path to utils directory\n",
    "    expert_ckpt_path = \"./utils\"\n",
    "\n",
    "    # Load expert config from utils/config.json\n",
    "    expert_config_file = os.path.join(expert_ckpt_path, \"config.json\")\n",
    "    if not os.path.exists(expert_config_file):\n",
    "        print(f\"Expert config file not found at {expert_config_file}\")\n",
    "        return None\n",
    "\n",
    "    with open(expert_config_file, 'r') as f:\n",
    "        expert_config = json.load(f)\n",
    "\n",
    "    # Create environment-specific checkpoint directory\n",
    "    ckpt_path = os.path.join(ckpt_path, env_name)\n",
    "    if not os.path.isdir(ckpt_path):\n",
    "        os.makedirs(ckpt_path)\n",
    "\n",
    "    print(f\"Setup complete - using expert config from {expert_config_file}\")\n",
    "    return expert_ckpt_path, expert_config, ckpt_path"
   ],
   "metadata": {
    "id": "6a08ebd1"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "47a4cda3",
   "cell_type": "markdown",
   "source": [
    "### **2. `load_and_save_configs(expert_ckpt_path, ckpt_path, env_name)`** (10 Points)\n",
    "- Loads a general configuration file (`config.json`) that contains training hyperparameters specific to the environment.\n",
    "- Saves these configurations to the current checkpoint folder for logging and reproducibility.\n",
    "\n",
    "**Purpose:** Fetch and store hyperparameters needed for training the GAIL model."
   ],
   "metadata": {
    "id": "47a4cda3"
   }
  },
  {
   "id": "9551c18e",
   "cell_type": "code",
   "source": [
    "def load_and_save_configs(expert_ckpt_path, ckpt_path, env_name):\n",
    "    # Load the config.json file from utils directory\n",
    "    config_file = os.path.join(expert_ckpt_path, \"config.json\")\n",
    "    if not os.path.exists(config_file):\n",
    "        print(f\"Config file not found at {config_file}\")\n",
    "        return None\n",
    "\n",
    "    with open(config_file, 'r') as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    # Save the config to the checkpoint directory for reproducibility\n",
    "    config_save_path = os.path.join(ckpt_path, \"config.json\")\n",
    "    with open(config_save_path, 'w') as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "\n",
    "    print(f\"Configuration loaded from {config_file} and saved to {config_save_path}\")\n",
    "    return config"
   ],
   "metadata": {
    "id": "9551c18e"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "57da1775",
   "cell_type": "markdown",
   "source": [
    "### **3. `setup_environment(env_name)`** (10 Points)\n",
    "\n",
    "- Initializes the chosen environment using OpenAI Gym.\n",
    "- Extracts the dimensions of the state and action spaces.\n",
    "- Checks whether the action space is discrete (e.g., CartPole) or continuous (e.g., Pendulum).\n",
    "\n",
    "**Purpose:** Provide key environment information for model initialization.\n"
   ],
   "metadata": {
    "id": "57da1775"
   }
  },
  {
   "id": "9305537e",
   "cell_type": "code",
   "source": [
    "def setup_environment(env_name):\n",
    "    # Create the environment\n",
    "    env = gym.make(env_name)\n",
    "\n",
    "    # Get state dimension\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "\n",
    "    # Check if action space is discrete or continuous and get action dimension\n",
    "    if hasattr(env.action_space, 'n'):  # Discrete action space\n",
    "        action_dim = env.action_space.n\n",
    "        discrete = True\n",
    "    else:  # Continuous action space\n",
    "        action_dim = env.action_space.shape[0]\n",
    "        discrete = False\n",
    "\n",
    "    return env, state_dim, action_dim, discrete"
   ],
   "metadata": {
    "id": "9305537e"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "45678fbc",
   "cell_type": "markdown",
   "source": [
    "### **4. `setup_device()`** (+5 Points for running on GPU)\n",
    "- Checks if a GPU (`cuda`) is available.\n",
    "- Falls back to CPU if GPU is not detected.\n",
    "\n",
    "**Purpose:** Ensure the training runs on the optimal hardware available.\n"
   ],
   "metadata": {
    "id": "45678fbc"
   }
  },
  {
   "id": "97a6533c",
   "cell_type": "code",
   "source": [
    "def setup_device():\n",
    "    # Check if CUDA is available and set device accordingly\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"CUDA not available. Using CPU.\")\n",
    "\n",
    "    return device"
   ],
   "metadata": {
    "id": "97a6533c"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "b1c6e95b",
   "cell_type": "markdown",
   "source": [
    "### **5. `initialize_expert(...)`** (15 Points)\n",
    "\n",
    "- Initializes an `Expert` model using its architecture config and environment dimensions.\n",
    "- Loads pre-trained policy weights (`policy.ckpt`) from the expert checkpoint.\n",
    "\n",
    "**Purpose:** Prepare a high-performance expert model whose behavior GAIL will learn to imitate."
   ],
   "metadata": {
    "id": "b1c6e95b"
   }
  },
  {
   "id": "7cde7652",
   "cell_type": "code",
   "source": [
    "def initialize_expert(expert_ckpt_path, state_dim, action_dim, discrete, expert_config, device, env_name):\n",
    "    # Get the environment-specific config\n",
    "    if env_name in expert_config:\n",
    "        train_config = expert_config[env_name]\n",
    "    else:\n",
    "        print(f\"No configuration found for environment: {env_name}\")\n",
    "        return None\n",
    "\n",
    "    # Initialize the Expert model with the correct parameters\n",
    "    expert = Expert(\n",
    "        state_dim=state_dim,\n",
    "        action_dim=action_dim,\n",
    "        discrete=discrete,\n",
    "        train_config=train_config\n",
    "    ).to(device)\n",
    "\n",
    "    # Load the policy checkpoint for the expert\n",
    "    policy_checkpoint_path = os.path.join(expert_ckpt_path, \"policy.ckpt\")\n",
    "    if not os.path.exists(policy_checkpoint_path):\n",
    "        print(f\"Expert policy checkpoint not found at {policy_checkpoint_path}\")\n",
    "        return None\n",
    "\n",
    "    # Load the trained weights\n",
    "    expert.pi.load_state_dict(torch.load(policy_checkpoint_path, map_location=device))\n",
    "    expert.eval()  # Set to evaluation mode\n",
    "\n",
    "    print(f\"Expert model loaded from {policy_checkpoint_path}\")\n",
    "    return expert"
   ],
   "metadata": {
    "id": "7cde7652"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "95af9d8f",
   "cell_type": "markdown",
   "source": [
    "### **6. `initialize_gail_model(...)`** (15 Points)\n",
    "- Initializes the `GAIL` model using environment specs and configuration parameters.\n",
    "- The model includes a policy, value function, and discriminator.\n",
    "\n",
    "**Purpose:** Set up the GAIL agent that learns by interacting with the environment and distinguishing between expert and learner behavior.\n"
   ],
   "metadata": {
    "id": "95af9d8f"
   }
  },
  {
   "id": "b86d7ec1",
   "cell_type": "code",
   "source": [
    "def initialize_gail_model(state_dim, action_dim, discrete, config, device):\n",
    "    # Initialize the GAIL model with the correct parameters\n",
    "    model = GAIL(\n",
    "        state_dim=state_dim,\n",
    "        action_dim=action_dim,\n",
    "        discrete=discrete,\n",
    "        train_config=config\n",
    "    ).to(device)\n",
    "\n",
    "    print(f\"GAIL model initialized with state_dim={state_dim}, action_dim={action_dim}, discrete={discrete}\")\n",
    "    return model"
   ],
   "metadata": {
    "id": "b86d7ec1"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "3967f575",
   "cell_type": "markdown",
   "source": [
    "### **7. `train_gail_model(model, env, expert)`** (20 Points)\n",
    "- Runs the training loop for the GAIL agent.\n",
    "- The GAIL model tries to imitate the expert by fooling the discriminator into thinking the learner’s actions come from the expert.\n",
    "\n",
    "**Purpose:** Train the policy using imitation learning via adversarial training.\n"
   ],
   "metadata": {
    "id": "3967f575"
   }
  },
  {
   "id": "bb6cc0c0",
   "cell_type": "code",
   "source": [
    "def train_gail_model(model, env, expert):\n",
    "    # Verify the training loop is functioning as expected\n",
    "    results = model.train(env, expert)\n",
    "    return results"
   ],
   "metadata": {
    "id": "bb6cc0c0"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "a94c5b6d",
   "cell_type": "markdown",
   "source": [
    "### **8. `save_results_and_checkpoints(...)`** (+5 Points)\n",
    "- Saves training results (e.g., episode returns) as a pickle file.\n",
    "- Saves the weights of the trained components: policy, value network, and discriminator.\n",
    "\n",
    "**Purpose:** Store the final model and results for evaluation and future use.\n"
   ],
   "metadata": {
    "id": "a94c5b6d"
   }
  },
  {
   "id": "0e18dafa",
   "cell_type": "code",
   "source": [
    "def save_results_and_checkpoints(ckpt_path, results, model):\n",
    "    # Save training results as a pickle file\n",
    "    results_path = os.path.join(ckpt_path, \"results.pkl\")\n",
    "    with open(results_path, 'wb') as f:\n",
    "        pickle.dump(results, f)\n",
    "    print(f\"Training results saved to {results_path}\")\n",
    "\n",
    "    # Save model weights for each component\n",
    "    # Save policy network\n",
    "    policy_path = os.path.join(ckpt_path, \"policy.ckpt\")\n",
    "    torch.save(model.pi.state_dict(), policy_path)\n",
    "\n",
    "    # Save value network\n",
    "    value_path = os.path.join(ckpt_path, \"value.ckpt\")\n",
    "    torch.save(model.v.state_dict(), value_path)\n",
    "\n",
    "    # Save discriminator network\n",
    "    discriminator_path = os.path.join(ckpt_path, \"discriminator.ckpt\")\n",
    "    torch.save(model.d.state_dict(), discriminator_path)\n",
    "\n",
    "    print(f\"Model checkpoints saved:\")\n",
    "    print(f\"  - Policy: {policy_path}\")\n",
    "    print(f\"  - Value: {value_path}\")\n",
    "    print(f\"  - Discriminator: {discriminator_path}\")"
   ],
   "metadata": {
    "id": "0e18dafa"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "3c10ba5b",
   "cell_type": "markdown",
   "source": [
    "### 9. **`main()`** (25 Points)\n",
    "- The main function orchestrates the entire GAIL training process.\n",
    "- It calls all the above functions in sequence to set up directories, load configurations, initialize models, train the GAIL agent, and save results.\n"
   ],
   "metadata": {
    "id": "3c10ba5b"
   }
  },
  {
   "id": "7ca58aa5",
   "cell_type": "code",
   "source": [
    "def main(env_name):\n",
    "    # Set up directories and validate environment\n",
    "    expert_ckpt_path, expert_config, ckpt_path = setup_directories(env_name)\n",
    "    if not expert_ckpt_path:\n",
    "        return\n",
    "    # Load and save configuration files\n",
    "    config = load_and_save_configs(expert_ckpt_path, ckpt_path, env_name)\n",
    "    # Set up environment and get dimensions\n",
    "    env, state_dim, action_dim, discrete = setup_environment(env_name)\n",
    "    # Set up the device (CPU or GPU)\n",
    "    device = setup_device()\n",
    "    # Initialize expert model\n",
    "    expert = initialize_expert(expert_ckpt_path, state_dim, action_dim, discrete, expert_config, device, env_name)\n",
    "    if expert is None:\n",
    "        print(\"Failed to initialize expert model. Exiting.\")\n",
    "        return\n",
    "    # Initialize GAIL model\n",
    "    model = initialize_gail_model(state_dim, action_dim, discrete, config[env_name], device)\n",
    "    # Train the GAIL model\n",
    "    results = train_gail_model(model, env, expert)\n",
    "    # Close the environment\n",
    "    env.close()\n",
    "    # Save results and model checkpoints\n",
    "    save_results_and_checkpoints(ckpt_path, results, model)"
   ],
   "metadata": {
    "id": "7ca58aa5"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "33ca5295",
   "cell_type": "markdown",
   "source": [
    "### 10. **`run_gail(env_name)`** (+5 Points if using parse_args)\n",
    "- This function serves as the entry point for running the GAIL training pipeline.\n",
    "- It accepts an environment name as input and executes the entire GAIL training process.\n"
   ],
   "metadata": {
    "id": "33ca5295"
   }
  },
  {
   "id": "2c3c91df",
   "cell_type": "code",
   "source": [
    "# Fix for newer NumPy versions\n",
    "if not hasattr(np, 'bool8'):\n",
    "    np.bool8 = np.bool_\n",
    "if not hasattr(np, 'int0'):\n",
    "    np.int0 = np.intp\n",
    "\n",
    "# Ensure that the command-line arguments are parsed correctly\n",
    "parser = argparse.ArgumentParser(description='Train GAIL model with expert policy')\n",
    "parser.add_argument('--env', type=str, default='CartPole-v1',\n",
    "                    choices=['CartPole-v1', 'Pendulum-v0', 'BipedalWalker-v3'],\n",
    "                    help='Environment name for GAIL training')\n",
    "\n",
    "# For Jupyter notebook, we need to handle the case where no command line args exist\n",
    "try:\n",
    "    args = parser.parse_args()\n",
    "except SystemExit:\n",
    "    # If running in Jupyter, use default arguments\n",
    "    args = argparse.Namespace(env='CartPole-v1')\n",
    "\n",
    "# Run the Main Function\n",
    "# Verify that the correct environment name is passed to the main function\n",
    "main(args.env)"
   ],
   "metadata": {
    "id": "2c3c91df",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "84727232-3166-48fa-888f-aff4f376c22b"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "e071ed43",
   "cell_type": "markdown",
   "source": [
    "provide a plot like below:"
   ],
   "metadata": {
    "id": "e071ed43"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Load the results from the saved pickle file\n",
    "results_path = os.path.join(\"ckpts\", \"CartPole-v1\", \"results.pkl\")\n",
    "with open(results_path, \"rb\") as f:\n",
    "    results = pickle.load(f)\n",
    "\n",
    "# Extract results from GAIL training and rename variables\n",
    "expert_reward_mean, gail_reward_iterations = results\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot the GAIL training progress\n",
    "iterations = range(1, len(gail_reward_iterations) + 1)\n",
    "plt.plot(iterations, gail_reward_iterations, 'b-', linewidth=2, marker='o', markersize=6, label='GAIL Agent Reward')\n",
    "\n",
    "# Add horizontal line for expert performance\n",
    "plt.axhline(y=expert_reward_mean, color='r', linestyle='--', linewidth=2, label=f'Expert Reward (Mean: {expert_reward_mean:.2f})')\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Training Iterations', fontsize=12)\n",
    "plt.ylabel('Average Reward', fontsize=12)\n",
    "plt.title('GAIL Training Progress vs Expert Performance', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add some styling\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(f\"\\nTraining Summary:\")\n",
    "print(f\"Expert Average Reward: {expert_reward_mean:.2f}\")\n",
    "print(f\"GAIL Final Average Reward: {gail_reward_iterations[-1]:.2f}\")\n",
    "print(f\"Performance Gap: {abs(expert_reward_mean - gail_reward_iterations[-1]):.2f}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 894
    },
    "id": "Yc1cwv1k-Hjg",
    "outputId": "b19fc9d4-8f6e-4391-d785-4cd3344b75ed"
   },
   "id": "Yc1cwv1k-Hjg",
   "outputs": [],
   "execution_count": null
  }
 ]
}
